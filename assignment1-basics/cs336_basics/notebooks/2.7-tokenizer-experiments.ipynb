{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# 2.7 Tokenizer Experiments (4 pts)\n", "\n", "**Problem (tokenizer_experiments):**\n", "- (a) Compression ratio for TinyStories and OWT tokenizers on 10 sampled docs?\n", "- (b) What happens tokenizing OWT with TinyStories tokenizer?\n", "- (c) Throughput (bytes/sec)? Time to tokenize The Pile (825GB)?\n", "- (d) Serialize tokenized datasets as `uint16`. Why is `uint16` appropriate?"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# (a) Compression ratios"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# (b) Cross-dataset tokenization"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# (c) Throughput benchmarks"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# (d) uint16 serialization"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## My Answers\n", "\n", "**(a):**\n", "\n", "**(b):**\n", "\n", "**(c):**\n", "\n", "**(d):**"]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.12.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}