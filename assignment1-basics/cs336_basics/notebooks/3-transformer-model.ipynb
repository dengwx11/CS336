{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# 3. Transformer Language Model\n", "\n", "Implementation notebook for all model components.\n", "Clean code goes in `cs336_basics/model.py`."]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 3.4 Linear & Embedding (2 pts)\n", "- `uv run pytest -k test_linear`\n", "- `uv run pytest -k test_embedding`"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Linear and Embedding development"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 3.5.1 RMSNorm (1 pt)\n", "$$\\text{RMSNorm}(a_i) = \\frac{a_i}{\\text{RMS}(a)} g_i$$\n", "- `uv run pytest -k test_rmsnorm`"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# RMSNorm development"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 3.5.2 SwiGLU FFN (2 pts)\n", "$$\\text{FFN}(x) = W_2(\\text{SiLU}(W_1 x) \\odot W_3 x)$$\n", "- `uv run pytest -k test_swiglu`"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# SwiGLU development"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 3.5.3 RoPE (2 pts)\n", "- `uv run pytest -k test_rope`"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# RoPE development"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 3.5.4 Attention (6 pts)\n", "- `uv run pytest -k test_softmax_matches_pytorch`\n", "- `uv run pytest -k test_scaled_dot_product_attention`\n", "- `uv run pytest -k test_4d_scaled_dot_product_attention`"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Softmax and Attention development"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 3.5.5 Multi-Head Self-Attention (5 pts)\n", "- `uv run pytest -k test_multihead_self_attention`"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# MHA development"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 3.6 Transformer Block & LM (6 pts)\n", "- `uv run pytest -k test_transformer_block`\n", "- `uv run pytest -k test_transformer_lm`"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Transformer block and full LM development"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 3.6 Resource Accounting (5 pts)\n", "\n", "**Problem (transformer_accounting):**\n", "- GPT-2 XL: 50257 vocab, 1024 ctx, 48 layers, 1600 d_model, 25 heads, 6400 d_ff"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Parameter counting and FLOP calculations"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## My Answers (Accounting)\n", "\n", "**(a) Params & memory:**\n", "\n", "**(b) FLOPs:**\n", "\n", "**(c) Most FLOPs:**\n", "\n", "**(d) GPT-2 small/medium/large:**\n", "\n", "**(e) Context 16384:**"]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.12.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}